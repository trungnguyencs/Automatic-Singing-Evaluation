{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trung Nguyen - 111752939\n",
    "## Assignment #3: Data Integration and Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (10%)\n",
    "\n",
    "> - Build a scoring function to rank houses by “desirability”, presumably a notion related to cost or value.  \n",
    "- Identify what the ten most desirable and least desirable houses in the Kaggle data set are.\n",
    "- Write a one page description of which variables your function used and how well you think it worked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "\n",
    "df = pd.read_csv('./data/properties_2017.csv', low_memory=False)\n",
    "# Calculate mean value and replace NaN values with mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are fields whose values are numercal but they should be listed as categorical, since their numerical values does not reflex the level of affection to the model (Such as ``typeid`` fields) \n",
    "\n",
    "Changing each of their value to a new binary feature would be the right way to extract information from them, but then the model becomes very complex with too many features, so I simply drop them.\n",
    "\n",
    "I also drop all binary fields, as I don't have experience with handling both categorical features and numerical features in the same model.\n",
    "\n",
    "I also fill the NaN values with 0 to avoid dealing with NaN values in finding the most positive and negative correlations\n",
    "\n",
    "(Actually I think it would be better to fill the matrix with median values. I tried to fill all the fields with its median values, however, the computation was too much that my laptop couldn't handle it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dropped = df.drop([\"hashottuborspa\", \"propertycountylandusecode\", \"propertyzoningdesc\", \n",
    "                            \"fireplaceflag\", \"taxdelinquencyflag\", \"assessmentyear\", \n",
    "                            \"airconditioningtypeid\", \"buildingclasstypeid\", \"decktypeid\",\n",
    "                            \"heatingorsystemtypeid\", \"regionidcity\", \"regionidcounty\",\n",
    "                            \"poolcnt\", \"fips\", \"buildingqualitytypeid\", \"threequarterbathnbr\",\n",
    "                            \"regionidneighborhood\", \"regionidzip\", \"storytypeid\",\n",
    "                            \"pooltypeid7\", \"rawcensustractandblock\", \"censustractandblock\",\n",
    "                            \"typeconstructiontypeid\", \"pooltypeid10\", \"pooltypeid2\", \"architecturalstyletypeid\",\n",
    "                            \"bathroomcnt\", \"propertylandusetypeid\"], axis=1)\n",
    "df_dropped_filled0 = df_dropped.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2985217, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dropped_filled0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parcelid                          int64\n",
       "basementsqft                    float64\n",
       "bedroomcnt                      float64\n",
       "calculatedbathnbr               float64\n",
       "finishedfloor1squarefeet        float64\n",
       "calculatedfinishedsquarefeet    float64\n",
       "finishedsquarefeet12            float64\n",
       "finishedsquarefeet13            float64\n",
       "finishedsquarefeet15            float64\n",
       "finishedsquarefeet50            float64\n",
       "finishedsquarefeet6             float64\n",
       "fireplacecnt                    float64\n",
       "fullbathcnt                     float64\n",
       "garagecarcnt                    float64\n",
       "garagetotalsqft                 float64\n",
       "latitude                        float64\n",
       "longitude                       float64\n",
       "lotsizesquarefeet               float64\n",
       "poolsizesum                     float64\n",
       "roomcnt                         float64\n",
       "unitcnt                         float64\n",
       "yardbuildingsqft17              float64\n",
       "yardbuildingsqft26              float64\n",
       "yearbuilt                       float64\n",
       "numberofstories                 float64\n",
       "structuretaxvaluedollarcnt      float64\n",
       "taxvaluedollarcnt               float64\n",
       "landtaxvaluedollarcnt           float64\n",
       "taxamount                       float64\n",
       "taxdelinquencyyear              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dropped_filled0.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (10%)\n",
    "\n",
    "> - Define a house “pairwise distance function”, which measures the similarity of two properties.  Like a distance metric, similar pairs of very similar properties should be distance near zero, with distance increasing as the properties grow more dissimilar.  \n",
    "- Experiment with your distance function, and write a one page discussion evaluating how well you think it worked.   Your function should include geographic as well as property-specific variables. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (15%)\n",
    "\n",
    "> - Using your distance function and an appropriate clustering algorithm, cluster the houses using your distance function into 10 to 100 classes, as you see best.   \n",
    "- Present a dot-plot/map (with tiny dots colored to reflect the clustering) illustrating the clusters your method produced.    \n",
    "- Write a one page discussion/analysis of what your clusters seem to be capturing, and how well they work \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (20%)\n",
    "\n",
    "> - Identify at least one external data set which you can integrate into your price prediction analysis to make it better.  Perhaps it can be financial, such as the historical effects of interest rates, consumer confidence, etc. on housing prices.  Perhaps it can be geographic, like the crime rate, educational scores, income levels, etc.   \n",
    "- Write a one page discussion/analysis on whether this data helps with the prediction tasks \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (20%)\n",
    "\n",
    "> - Finally, build the best prediction model you can to solve the Zillow task.  Use any data, ideas, and approach that you like. Predict the logerror for instances at file “sample_submission.csv”.  \n",
    "- Report the score/rank you get. \n",
    "- Write a 2-3 page report about how it works, an evaluation, and any interesting experiences along the way.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 (15%)\n",
    "\n",
    "> - Do a permutation test to determine a p-value of how good your predictions of logerror are.  You can use whatever metric you wish to score your model (like mean absolute error).   \n",
    "- For a large enough sample of the evaluation data, compare how your model ranks by this metric on the real data compared to 100 (or more) random permutations of the logerror assigned to the real data records. \n",
    "- What fraction of permutations produce at least error at least as good at the real data set?   If necessary, sample your data so these 100+ runs do not take too much time. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 (10%)\n",
    "\n",
    "> Submit your results on the real test data to Kaggle before deadline. Write the result into a csv file and submit it to the website. Actually, submit two for your two best models, to the extent that Zillow allows it.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
