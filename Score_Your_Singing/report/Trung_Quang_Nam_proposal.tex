%\documentclass{article}
\documentclass[paper=a4, fontsize=11pt]{scrartcl}

\usepackage{graphicx}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage[T5]{fontenc}                           
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\begin{document}

\title{The Stony Brook Glad All Over Machine\\ Project Proposal}
\author{Trung Nguyen - 111752939
\\Anh Quang Do - 110922124
\\Nam Nguyen - 111171365}
\date{\today}
\maketitle
  
\section{Background}
\subsection{Music information retrieval (MIR)}

MIR is an interdisciplinary research that lies on the edges of musicology, psychology, signal processing, and machine learning. 
%Applications of MIR can be listed as follows:
%
%\begin{itemize}
%\item Music recommendation and annotation
%\item Automatic classification of genre, mood, artists, etc.
%\item Automatic music transcription
%\item Music generation
%\end{itemize}

%In our project, we focus on learning to rank/group the performance of vocal singing.

Basic of a MIR system, including segmentation, feature extraction, analysis is represented as in the figure below.
\begin{center}
\includegraphics[scale=0.2]{img/method.png}\\
\textit{Basic of a MIR system}\\
\end{center}

\subsection{Automatic Evaluation of Karaoke Singing}
%Karaoke is a popular form of singing entertainment, rooted from Japan. 
Many of nowaday karaoke systems have a scoring feature to evaluate singers' performance. However, these rating is poorly constructed and not matched with human rating. 
%Though speech processing is a very developed field of studies and techonologies, its approach is not suitable for singing voice because singing voice differs from speech in its intensity and dynamic range and because singing voice usually comes with a non stationary background music signal.

In this project, we focus on the vocal quality of the singing and train the machine to distinguish between good and poor singing. This can be done by recalling features of the singing, such as enthusiasm, emotion, pitch, volume, rythm, melodic similarity measures, etc.

\subsection{Feature representation}
Music analysis often requires some summarising and is achieved by feature extraction. One common feature extracted is the Mel-Frequency Cepstral Coefficient (MFCC).

\subsubsection{Feature extraction}

%To evaluate a piece of singing, we would like to combine both the score-based and machine-learned ranking methods. We first identify the perception which makes a piece of singing good or bad; it can be the enthusiasm of singers, which is mostly represented by intensity of the singings, or it can be the pitch interval, the vibrato, etc., or both. Then, we then link this perception to appropriate acoustic features. Alternatively, we can do a feature importance analysis among these acoustic features.

There are many features that can be extracted from music signal. These features can be categorized into: reference features, content-based features and text-based features. Reference features can be those relating to social interactions, e.g. followers, performance rating in, for example, soundcloud. Text-based features includes lyrics, interview, etc. Our approach will be based on content-based features, extracted from the wave signal, e.g. pitch, rythm, etc. and we might use reference features as ground truth for our machine-learned ranking method.

\subsubsection{Mel-Frequency Cepstral Coefficient (MFCC) }
The content-based features are calculated from low-level signal features (sometime refered as extraction methods), the most important of which is MFCC. MFCC and its derived features (such as "anchor space"[1]) have been shown to give good performance for a variety of audio classification tasks. MFCCs capture the short-time spectral shape, which carries information about instrumental timbres or the quality of a singing. 

%MFCCs are commonly derived as follows:

%\begin{itemize}
%
%\item Take the Fourier transform of (a windowed excerpt of) a signal.
%\item Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows.
%\item Take the logs of the powers at each of the mel frequencies.
%\item Take the discrete cosine transform of the list of mel log powers, as if it were a signal.
%\end{itemize}

MFCCs are increasingly finding uses in music information retrieval applications such as genre classification, audio similarity measures, etc.

%\subsubsection{Music perception}
%%Human can easily distinguish between music and noise. Physically, music is ordered and patterned sound waves, produced by harmonic vibrations from the source. 
%
%The major attibutes of music are as follows:
%
%\begin{itemize}
%\item The pitch value is determined by the wavelength of the sound signal. High-pitched tones have short wavelengths, while low-pitched tones have long wavelengths.
%\item Intensity, or loudness is to the magnitude of acoustic waves. High-magnitude sound carries more energy than low-magnitude sound.
%item Duration refers to the amount of time in which a musical note sustain.
%\item Timbre refers to different harmonics, created by different source, e.g. piano vs. violin. 
%\end{itemize}


\section{Related work}
Music similarity metrics is a research aiming to calculate the similarity between songs or artists, comparing their performance. In [1], authors employ a feature, derived from MFCC, called 'anchor space', which uses musical categories and well-known anchor artists as convenient reference points for describing features of the music. It is inspired by a fold wisdom such as "Jeff Buckley sounds like Van Morrison meets Led Zeppelin, but more folky." Other approaches for song similarity are to embed songs into a Euclidian metric space and do some distance-based analysis and clustering [3].

For evaluation of karaoke singing, there is an approach based on the perception of singing enthusiasm [4]. The authors argue that karaoke is the form of entertainment for amateur so enthusiasm is a good criteria to evaluate them. They identified three acoustic features relevant to such perception: A weighted power, "fall-down", and vibrato extent, developed a system for evaluating singing enthusiasm, and obtained a correlation coefficient of 0.65 between the system output and human evaluation. In our point of view, their method can be considered as score-based ranking. In [5], the authors proposed a score combination from pitch-based, volume-based, and rhythm-based rating, with a reference specified karaoke song to evaluate a piece of singing. This approach is also score-based ranking. In [6], the authors used HMM as a statistical music recognition model for automatic scoring of karaoke computer games. The musical features they employed are Pitch \& Pitch Error, Accent, Zero-Crossing Rate, Root-Mean-Squared Energy.

Another effort for analyzing the singing voice is made in [7], in which the authors reported good results of a system for classifying "good" and "poor" singing based on SVM. In [8], the authors proposed a categorization and segmentation system for singing voice expression using pre-defined rules and HMM. There is another approach for automatic scoring of singing voice based on melodic similarity measures [9]. In [10], a method of evaluating singing skills that does not require score information is represented. The authors used pitch interval accuracy and vibrato as acoustic features to evaluate singing. The approach was then tested by a 2-class (good/poor) classification test with 600 song sequences, and achieved an average classification rate of 83.5\%. There is an approach for song classification based on perception of emtion [11].

\section{Our Approach}

\subsection{Dataset}
This project requires a large amount of audio data. For this reason, we visited several karaoke websites to look for recording data and among them, \textit{Redkaraoke} was found to be one of the largest sites for online recording worldwide which was created in 2007 and has more than 70000 songs (normally a karaoke website has ~15000 songs). Its users are also very diverse in regions. Therefore we think this website can provide good data for our purpose. 

We first chose a song, which is \textit{"My heart will go on"} by Celine Dion for our experiment. The reasons behind this choice is that it is one of the most popular and most recorded songs, its level of difficulty is relatively high, and we all like this song so we don't mind hearing it over and over again. The url for this song is:  \url{https://www.redkaraoke.com/karaoke/celine-dion/my-heart-will-go-on/11827}

We then built a scraper using \textit{BeautifulSoup4} library to extract the recordings and users' information. For this \textit{"My heart will go on"} song, we were able to sucessfully extracted around  4000 recordings, around half are .mp3 files and the rest are .mp4 files. We first downloaded all the .mp3 files, which take around 7.5GB of data storage. The scraper we built can extract information for other songs as well, but we think this amount of data is enough for us to experiment and we can always extract information for more songs whenever needed.

Aside from the audio files, text information from the recordings and the users were also extracted and saved as a csv file shown below:
 
 \begin{center}
\includegraphics[scale=0.235]{img/1.png}\\
\textit{Extracted songs and users' information}\\
\end{center}

\subsection{Tools and libraries}

There are many tools available for audio analysis in python, many of which are available to be evaluated in [2]. Among them, we chose the \textit{librosa} library to experiment because of its instruction availability. 


\subsection{Method and Experiment}

Our workflow follows the \textit{Music information retrieval process} presented in section 1.1: First we segment the music (since each audio file has different length) then extract the feature and finally use machine learning methods to evaluate/group the recordings.

We had experimented extracting the 20 MFCC features from a sample audio file in our dataset. We will continue to find more information about these features to decide which are useful for our problems.

\begin{center}
\includegraphics[scale=0.27]{img/2.png}\\
\textit{Heat map of 20 MFCC features extracted from a sample audio file in our dataset}\\
\end{center}

Once this step is finished, we may experiment building our model based Metric Learning to Rank [12] to automatically learn the distance metric, and compare it with distance in Euclidian metric space. This distance metric that have been learned will then be used in a learning to rank algorithm, based on Structural SVM.

Besides we want to extract the information not only from the audio files (content-features), but also from the information in the we retrieved (reference feature). These features may act as the groundtruth for our model. One metric which we find interesting is the number of likes.  Below is the statistics for the average number of likes in each time frame of 100 days. Except from some outliers (which turns out to be good singers who records several recordings in a short time), the average numbers of like is quite stable. From this analysis, we may conclude that this information may not heavily dependent on the recording's age, and therefore the number of likes may be good good for evaluating whether a recording is good or bad.

\begin{center}
\includegraphics[scale=0.3]{img/3.jpg}\\
\textit{Average number of likes for the recordings in each time frame of 100 days}\\
\end{center}

Separating the vocal and non-vocal segments of song as a preprocessing step is also considered. However this turns out to be a hard problem: as oppose to professional multi-channel audio recordings, our mp3 files only have 1-2 channels which is very difficult to separate the vocal and non-vocal segments. We will continue to find if there is a solution for this problem.

\subsection{Evaluation}

Evaluation whether the score given a user's recording is good or bad is a hard task since it highly depends on human's sensuality. In the materials that we have read, the most common way to evaluation is asking for evaluation from music professionals and then compare the score they give with the scores from the system[5][7]. 

From this, we think our model can be evaluated by doing a survey among a group of ~20 people of how they grade of how good the singer is in a scale from 1-5 and then compare the score from our system with the same scale.

\begin{thebibliography}{99}
\bibitem[1]{p3} Berenzweig, Adam, et al.
\newblock "A large-scale evaluation of acoustic and subjective music-similarity measures."
\newblock Computer Music Journal 28.2 (2004): 63-76.

\bibitem[2]{p3} Moffat, David, David Ronan, and Joshua D. Reiss.
\newblock "An evaluation of audio feature extraction toolboxes." (2015). Computer Music Journal 28.2 (2004): 63-76.

\bibitem[3]{p3} Slaney, Malcolm, Kilian Weinberger, and William White.
\newblock "An evaluation of audio feature extraction toolboxes." (2015).
\newblock "Learning a metric for music similarity." International Symposium on Music Information Retrieval (ISMIR). 2008.

\bibitem[4]{p3} Daido, Ryunosuke, et al. "A System for Evaluating Singing Enthusiasm for Karaoke." ISMIR. 2011.

\bibitem[5]{p3} Tsai, Wei-Ho, and Hsin-Chieh Lee. "Automatic evaluation of karaoke singing based on pitch, volume, and rhythm features." IEEE Transactions on Audio, Speech, and Language Processing 20.4 (2012): 1233-1243.

\bibitem[6]{p3} Qiu, Dongping. "Development of Scoring Algorithm for Karaoke Computer Games." (2012).

\bibitem[7]{p3} Nakano, Tomoyasu, Masataka Goto, and Yuzuru Hiraga. "An automatic singing skill evaluation method for unknown melodies using pitch interval accuracy and vibrato features." Ninth International Conference on Spoken Language Processing. 2006.

\bibitem[8]{p3} Mayor, Oscar, Jordi Bonada, and Alex Loscos. "The singing tutor: Expression categorization and segmentation of the singing voice." Proceedings of the AES 121st Convention. 2006.

\bibitem[9]{p3} Molina, Emilio, E. Gómez, and I. Barbancho. "Automatic scoring of singing voice based on melodic similarity measures." Master's thesis, Universitat Pompeu Fabra, Barcelona, Spain (2012): 9-14.

\bibitem[10]{p3} Nakano, Tomoyasu, Masataka Goto, and Yuzuru Hiraga. "An automatic singing skill evaluation method for unknown melodies using pitch interval accuracy and vibrato features." Ninth International Conference on Spoken Language Processing. 2006.

\bibitem[11]{p3} Eyben, Florian, et al. "Emotion in the singing voice—a deeperlook at acoustic features in the light ofautomatic classification." EURASIP Journal on Audio, Speech, and Music Processing 2015.1 (2015): 1-9.Universitat Pompeu Fabra, Barcelona, Spain (2012): 9-14.

\bibitem[12]{p3} McFee, Brian, and Gert R. Lanckriet. "Metric learning to rank." Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010.

\end{thebibliography}

\end{document}